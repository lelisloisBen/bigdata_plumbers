<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Samir Profile</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.1/css/bootstrap.min.css" integrity="sha384-VCmXjywReHh4PwowAiWNagnWcLhlEJLA5buUprzK8rxFgeH0kww/aWY76TfkUoSX" crossorigin="anonymous">

    <style>
        body {
            background-color: black;
        }
        .blue_text {
            color: blue;
        }
        .green_text {
            color: green;
        }
        .red_text {
            color: red;
            font-weight: bold;
        }
    </style>
</head>
<body>
    
    <div class="container">
        <div class="jumbotron mt-5">
            <p>
                (1) I'm a <b class="blue_text">Data Engineer</b> with 7 years experience,
                during my carier I've been 
                <br>
                    <i class="red_text">
                    - ingesting data from many sources <br>
                    - used different processing tools <br>
                    - also store my data with a large variety of databases.
                    </i>
            </p>
            <p>
                (2) I'm currently working as <b class="blue_text">Data engineer</b> at <u>The Hartford, hardfort Connecticut.</u>
                <br>
                Ingesting Data from the Company rest API and store them in relational database for <b class="blue_text">marketing purposes.</b>
                <br>
                <i class="red_text">
                -   I'm using Flume for ingest the Data to HDFS <br>
                -   Spark to perform transformation such as map or filter <br>
                -   MySQL to store the data. 
                </i>
                <br>
            </p>
            <p>
                Prior to that,
            </p>
            <p>
                (3) I worked as <b class="blue_text">AWS Cloud Data engineer</b> at <u>Progressive Corporation, Mayfield Ohio.</u>
                <br>
                We were <i class="red_text">Migrating Data</i> from a local data warehouse to <i class="red_text">AWS S3 bucket and Redshift</i>
            </p>
            <p>
                I've been <b class="blue_text">Excelling</b> in <br>
                <i class="red_text">
                -   Data Ingesting <br>
                -   Data processing also in <br> 
                -   Data migration <br>
                </i>
                during this years,
                <br>
                that's why I think I will perfectly fit for the position you offering.
            </p>
        </div>
    </div>

    <div class="container">
        <div class="jumbotron mt-5">
            (1) project
            <br>
            - <u>THE HARTFORD, Hartford, connecticut</u>
            <br>
            - <b class="blue_text">Data engineer</b>
            <br>
            - <b class="green_text">January 2019 - present.</b>
            <br>
            - Ingesting Data from the Company rest API and store them in relational database for marketing purposes.
            <br>
                To have a visualize later by the anilist team for customer services. (Tableau)
            <br>
            The Data were Historical data (costumer info like name, age,city, previous accident, what insurance plan they have, car insurance, house insurance...)
            The Analist team was doing visualization in tableau so they know where to increase or decrease insurance rates.
            
            <br> <br>
            <img src="project1.png" alt="pro" width="100%" height="100%">
            <br>

            <p>
                Pipeline -> Rest API -> Flume -> HDFS -> Spark -> MySQL or HIVE <br>
                JSON format, spark-streaming, PySpark, map() flatMap() filter() groupby() <br>
                JDBC driver in PySpark to connect MySQL Database <br>
                Dataframe -> tabular format <br>
                Airflow -> multiple spark Jobs at midnight (12am) <br>
                Spark SQL queries and optimized the Spark queries with Spark SQL. <br>
                ETL = Extract-Transform-Load || spark UDF = User-Defined Functions for SparkSQL <br>
                Hortonworks -> Hadoop Clusters -> Monitor using Ambari <br>
                Sqoop to Hive -> from mySQL to Hive -> dynamic partition (bucket size define) using staging tables. <br>
            </p>
            <a target="_blank" href="https://www.thehartford.com/about-us">The Hartford website ☝</a>
            || 
            <a target="_blank" href="https://goo.gl/maps/VhV373WN6wmyDHub7">Map</a>
            <br>
            Financial and insurance company, Fortune 500 in 2018<br>
            Christopher Swift CEO, Doug Elliot President, Beth Costello CFO <br>
            insurance company for individual and companies <br>
            Noth of New-York and South of Boston <br>
            Italian Rest Salute 10 blocks away / Kent pizza 1 block away.

            <br>
            <hr>
            (2) project
            <br>
            - <u>PROGRESSIVE CORP, Mayfield, Ohio</u>
            <br>
            - <b class="blue_text">AWS Cloud Data Engineer</b>
            <br>
            - <b class="green_text">May 2017 - January 2019</b>
            <br>
            - migrating data from local to AWS S3 bucket and Redshift
            <br>
            - <i class="red_text">AWS, S3, Redshift, Spark, Hive, Kafka</i>
            <br>
            <br>
            Data were mostly customer experiences and claims <br>
            <br>
            <img src="project2.png" alt="pro" width="100%" height="100%">

            <br>

            Designed and implemented test environment on AWS. <br> 
            EC2 Instance creation and Auto Scaling, snapshot backup and managing template. <br>
            Implemented Spark using Scala, and utilized DataFrames and Spark SQL API for faster processing of data. <br>
            Involved in converting HiveQL/SQL queries into Spark transformations using Spark RDDs, Python and Scala. <br>
            Used Spark streaming to receive real time data using Kafka <br>
            
            <br>
            <br>
            Insurance company, cleveland campus 1, Mayfield Village, Cuyahoga County Ohio. <br>
            Less than 30 minutes away from cleveland, close to Lake Erie.
            Noth between new-york and chicago, close to detroit and the canadian border.(Toronto closer canadian city)

            <br>
            <br>
            <hr>
            (3) project 
            <br>
            - <u>MCKESSON CORP, Irving, Texas</u>
            <br>
            - <b class="blue_text">Big Data Engineer</b>
            <br>
            - <b class="green_text">December 2015 - May 2017</b>
            <br>
            - 
            <br>
            - <i class="red_text">Spark, Kafka</i>
            <br>
            <br>

            Analyzed and documented existing customizations as well as the current SharePoint 2013 environment.
            • Used Spark-SQL and Hive Query Language (HQL) for getting customer insights, to be used for critical decision
            making by business users.
            • Developed a task execution framework using SQL and HiveQL.
            • Used Spark SQL and Data Frame API extensively to build Spark applications.
            • Used Spark engine, Spark SQL for data analysis and provided to the data scientists for further analysis.
            • Performed streaming data ingestion to the Spark distribution environment, using Kafka.
            • Closely worked with data science team in building Spark MLlib applications to build various predictive models.
            • Used Spark-Streaming APIs to perform necessary transformations and actions on the real-time data using Kafka
            • Kibana setup, dashboarding and visualization configuration.
            • Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing.
            • Configured Kafka Producer in Python to Ingest Data and send the Data to Broker Topic.
            • Created a Kafka Consumer in Python to see the ingestion of the data in the console. 

            <br>
            <br>
            Currently ranked 7th on the FORTUNE 500, we are a global leader in
            healthcare supply chain management solutions, retail pharmacy, healthcare
            technology, community oncology and specialty care. 
            5 minutes away from Dallas/Fort Worth International Airport || DFW Airport
            20 minutes from Dallas Texas.
            <br>
            <hr>
            (4) project
            <br>
            - <u>EPIC SYSTEM, Verona, Wisconsin</u>
            <br>
            - <b class="blue_text">Big Data Engineer</b>
            <br>
            - <b class="green_text">October 2014 - December 2015</b>
            <br>
            - Ingesting data from Oracle, manipulate and analize record for software integration.
            <br>
            - <i class="red_text">Hortonworks, Flume, Kafka, HDFS, Spark, Oracle, MySQL, Hbase</i>
            <br>
            <br>
            
            Health company, software & services, 
            <br>
            Founded in 1979, Epic build software to retrieve electronic record from patient. <br>
            Wisconsin up north Chicago, verona 10 miles from madison
            <br>
        </div>
    </div>
    


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js" integrity="sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.1/js/bootstrap.min.js" integrity="sha384-XEerZL0cuoUbHE4nZReLT7nx9gQrQreJekYhJD9WNWhH8nEW+0c5qq7aIo2Wl30J" crossorigin="anonymous"></script>
</body>
</html>